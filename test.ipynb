{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "81491370-2c1d-4dcd-94b3-96b49844b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import os\n",
    "\n",
    "from data_preprocess import *\n",
    "from model import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "87e3be56-8019-4660-8ae8-e9a3a6c27075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() and torch.backends.mps.is_built() else 'cpu')\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dcfb7749-794e-42e7-83e9-4abb6460036c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1, 32, 19, 24, 32, 34, 21, 13,  9,  6,  2])\n",
      "torch.Size([2, 11])\n",
      "vyavaharik\n",
      "व्यवहारिक\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'dakshina_dataset_v1.0'\n",
    "lang = 'hi'  # Hindi\n",
    "subfolder_dir = 'lexicons'\n",
    "train_path = os.path.join(data_dir, lang, subfolder_dir, f'{lang}.translit.sampled.train.tsv')\n",
    "dev_path = os.path.join(data_dir, lang, subfolder_dir, f'{lang}.translit.sampled.dev.tsv')\n",
    "test_path = os.path.join(data_dir, lang, subfolder_dir, f'{lang}.translit.sampled.test.tsv')\n",
    "\n",
    "train_loader, val_loader, test_loader, src_vocab, tgt_vocab = prepare_dataloaders(train_path, dev_path, test_path, batch_size=2, repeat_datapoints=True)\n",
    "\n",
    "# batch = next(iter(train_loader))\n",
    "counter = 0\n",
    "for src_batch, tgt_batch in train_loader:\n",
    "    src = src_batch[0]\n",
    "    tgt = tgt_batch[0]\n",
    "    print(tgt_batch[0])\n",
    "    print(tgt_batch.shape)\n",
    "    print(src_vocab.decode(src))\n",
    "    print(tgt_vocab.decode(tgt))\n",
    "    counter += 1\n",
    "    if counter>0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0f4b258a-c49c-4cb5-bc27-84b6092a223b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tgt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1e6aa6f3-13cf-4132-bdaf-ac0d246a15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "hidden_dim = 256\n",
    "num_layers = 1\n",
    "rnn_type = 'LSTM'  # can be 'RNN' or 'LSTM' or 'GRU'\n",
    "num_epochs = 2\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(input_dim=len(src_vocab), emb_dim=embedding_dim, hidden_dim=hidden_dim,\n",
    "                  num_layers=num_layers, rnn_type=rnn_type)#.to(device)\n",
    "decoder = Decoder(output_dim=len(tgt_vocab), emb_dim=embedding_dim, hidden_dim=hidden_dim,\n",
    "                  num_layers=num_layers, rnn_type=rnn_type)#.to(device)\n",
    "model = Seq2Seq(encoder, decoder, device)#.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.pad_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "924071fa-365a-4998-bf2c-74295b30773d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 11, 67])\n"
     ]
    }
   ],
   "source": [
    "model.encoder.train()\n",
    "model.decoder.train()\n",
    "total_loss = 0\n",
    "\n",
    "_, hidden = encoder(src_batch)\n",
    "input_token = tgt_batch[:,0]\n",
    "batch_size = src_batch.size(0)\n",
    "tgt_len = tgt_batch.size(1)\n",
    "output_dim = model.decoder.output_dim\n",
    "outputs = torch.zeros(batch_size, tgt_len, output_dim)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "79b3b004-ecc9-473f-bb3d-87a6be86b700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4]) — [batch_size, seq_len]\n",
      "Embedded shape: torch.Size([2, 4, 8]) — [batch_size, seq_len, emb_dim]\n",
      "RNN outputs shape: torch.Size([2, 4, 16]) — [batch_size, seq_len, hidden_dim]\n",
      "Hidden shape: torch.Size([2, 2, 16]) — [num_layers, batch_size, hidden_dim]\n",
      "Cell shape: torch.Size([2, 2, 16]) — [num_layers, batch_size, hidden_dim]\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "vocab_size = 10\n",
    "embedding_dim = 8\n",
    "hidden_dim = 16\n",
    "num_layers = 2\n",
    "cell_type = 'lstm'  # change to 'gru' or 'rnn' to test others\n",
    "\n",
    "# Dummy input (each number is a token index)\n",
    "src = torch.tensor([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 0]\n",
    "])  # shape: (batch_size, seq_len)\n",
    "\n",
    "print(f\"Input shape: {src.shape} — [batch_size, seq_len]\")\n",
    "\n",
    "# Embedding layer\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "embedded = embedding(src)\n",
    "print(f\"Embedded shape: {embedded.shape} — [batch_size, seq_len, emb_dim]\")\n",
    "\n",
    "# RNN cell\n",
    "if cell_type == 'lstm':\n",
    "    rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "elif cell_type == 'gru':\n",
    "    rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "else:\n",
    "    rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "# Forward pass\n",
    "if cell_type == 'lstm':\n",
    "    outputs, (hidden, cell) = rnn(embedded)\n",
    "else:\n",
    "    outputs, hidden = rnn(embedded)\n",
    "\n",
    "print(f\"RNN outputs shape: {outputs.shape} — [batch_size, seq_len, hidden_dim]\")\n",
    "print(f\"Hidden shape: {hidden.shape} — [num_layers, batch_size, hidden_dim]\")\n",
    "\n",
    "if cell_type == 'lstm':\n",
    "    print(f\"Cell shape: {cell.shape} — [num_layers, batch_size, hidden_dim]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f0d3271a-7ece-4cd4-9ca0-e08fba401b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token indices shape: torch.Size([2])\n",
      "Embedded shape: torch.Size([2, 1, 8]) — [batch_size, 1, emb_dim]\n",
      "RNN output shape: torch.Size([2, 1, 16]) — [batch_size, 1, hidden_dim]\n",
      "Hidden state shape: torch.Size([2, 2, 16]) — [num_layers, batch_size, hidden_dim]\n",
      "Prediction shape: torch.Size([2, 67]) — [batch_size, output_dim]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Config\n",
    "batch_size = 2\n",
    "vocab_size = 10\n",
    "embedding_dim = 8\n",
    "hidden_dim = 16\n",
    "output_dim = vocab_size  # usually same as target vocab size\n",
    "num_layers = 2\n",
    "cell_type = 'lstm'  # change to 'gru' or 'rnn' to test others\n",
    "\n",
    "# Dummy input token indices for a single time step\n",
    "input_token = torch.tensor([1, 5])  # shape: (batch_size,)\n",
    "print(f\"Input token indices shape: {input_token.shape}\")\n",
    "\n",
    "# Embedding layer\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "embedded = embedding(input_token.unsqueeze(1))  # shape: (batch_size, 1, emb_dim)\n",
    "print(f\"Embedded shape: {embedded.shape} — [batch_size, 1, emb_dim]\")\n",
    "\n",
    "# RNN cell\n",
    "if cell_type == 'lstm':\n",
    "    rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "elif cell_type == 'gru':\n",
    "    rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "else:\n",
    "    rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "# Dummy initial hidden state\n",
    "if cell_type == 'lstm':\n",
    "    hidden = (\n",
    "        torch.zeros(num_layers, batch_size, hidden_dim),  # h_n\n",
    "        torch.zeros(num_layers, batch_size, hidden_dim),  # c_n\n",
    "    )\n",
    "else:\n",
    "    hidden = torch.zeros(num_layers, batch_size, hidden_dim)  # h_n\n",
    "\n",
    "# Forward pass\n",
    "if cell_type == 'lstm':\n",
    "    output, (hidden, cell) = rnn(embedded, hidden)\n",
    "else:\n",
    "    output, hidden = rnn(embedded, hidden)\n",
    "\n",
    "print(f\"RNN output shape: {output.shape} — [batch_size, 1, hidden_dim]\")\n",
    "print(f\"Hidden state shape: {hidden.shape} — [num_layers, batch_size, hidden_dim]\")\n",
    "\n",
    "# Final linear projection to vocabulary\n",
    "fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "prediction = fc_out(output.squeeze(1))  # shape: (batch_size, output_dim)\n",
    "print(f\"Prediction shape: {prediction.shape} — [batch_size, output_dim]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bc1b8d8b-0ddf-41d8-b6e6-5f362459752b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "For batched 3-D input, hx and cx should also be 3-D but got (2-D, 2-D) tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, tgt_len):\n\u001b[0;32m----> 2\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m decoder(input_token, hidden)  \u001b[38;5;66;03m# output: (batch_size, output_dim)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     outputs[:, t] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m      5\u001b[0m     teacher_force \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/da6401/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/da6401/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/IITM/Academics/Deep Learning/DA6401_Assignment3/model.py:93\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size) → (batch_size, 1)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(\u001b[38;5;28minput\u001b[39m)  \u001b[38;5;66;03m# (batch_size, 1, emb_dim)\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(embedded, hidden)  \u001b[38;5;66;03m# output: (batch_size, 1, hidden_dim)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# (batch_size, output_dim)\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prediction, hidden\n",
      "File \u001b[0;32m~/miniconda3/envs/da6401/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/da6401/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/da6401/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1108\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m   1104\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor batched 3-D input, hx and cx should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1106\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malso be 3-D but got (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D) tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1107\u001b[0m         )\n\u001b[0;32m-> 1108\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: For batched 3-D input, hx and cx should also be 3-D but got (2-D, 2-D) tensors"
     ]
    }
   ],
   "source": [
    "for t in range(1, tgt_len):\n",
    "    output, hidden = decoder(input_token, hidden)  # output: (batch_size, output_dim)\n",
    "    outputs[:, t] = output\n",
    "    \n",
    "    teacher_force = torch.rand(1).item() < 0.2\n",
    "    top1 = output.argmax(1)  # (batch_size)\n",
    "    \n",
    "    input_token = tgt_batch[:, t] if teacher_force else top1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1b732c81-5750-46f1-a2db-93d3c29b3d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 67])\n",
      "torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "outputs = outputs[:, 1:].reshape(-1, output_dim)\n",
    "print(outputs.shape)\n",
    "tgt_batch = tgt_batch[:, 1:].reshape(-1)\n",
    "print(tgt_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d38a1ebb-2fc2-4090-bf4c-73f065fefafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 16])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (da6401)",
   "language": "python",
   "name": "da6401"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
